{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skkSZ6PTpl3T"
   },
   "outputs": [],
   "source": [
    "!pip install ultralytics deep_sort_realtime facenet-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "33bd89f72aa245ca952d9906f2e2fa2a",
      "8bff9a8eb21e4222aeb385f15eede244",
      "23a969fb6d824728aa9f504ede5819fa",
      "708d963d3e1649eebeb959e08a3f27fe",
      "b6341f97a93148678f585a67bbe8b2ea",
      "418f6dd8e50b4ef0ab96cbc2efffbe61",
      "a4d90527f24b41df8ee69f29f2c280c1",
      "287132a2e8be411982e21209a07db4dc",
      "8f8b152be5264b0da233f4f225dbf434",
      "8674158cd57b4f05b45663a2f043abb2",
      "4705eafb2871441588e8b30689670863"
     ]
    },
    "id": "eELDonCmJJBf",
    "outputId": "b4be3477-aad9-49b9-ff6a-b5e2b0ef51d3"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "\n",
    "# Google Colab用のimport文\n",
    "from google.colab.patches import cv2_imshow\n",
    "from google.colab import files\n",
    "\n",
    "# 必要なライブラリのインポートを試みる\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "except ImportError:\n",
    "    print(\"Error: Required libraries are not installed.\")\n",
    "    print(\"Please install them using: !pip install ultralytics deep_sort_realtime facenet-pytorch\")\n",
    "    exit(1)\n",
    "\n",
    "def enhance_image(image, brightness=0, contrast=0):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    image_tensor = torch.from_numpy(image).float().to(device)\n",
    "    image_tensor = image_tensor * (1 + contrast/100) + brightness\n",
    "    image_tensor = torch.clamp(image_tensor, 0, 255)\n",
    "    enhanced = image_tensor.byte().cpu().numpy()\n",
    "    return enhanced\n",
    "\n",
    "def process_video(video_path, yolo_model, tracker, mtcnn, resnet, output_dir, known_face_embeddings, start_unique_count):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Unable to open video file: {video_path}\")\n",
    "        return start_unique_count, known_face_embeddings, 0, 0, 0, 0\n",
    "\n",
    "    total_unique_person_count = start_unique_count\n",
    "    frame_count = 0\n",
    "    no_face_detected_count = 0\n",
    "    no_person_detected_count = 0\n",
    "    passersby_count = 0\n",
    "\n",
    "    yolo_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((640, 640)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    device = next(yolo_model.parameters()).device\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_count % 60 == 0:\n",
    "            current_time_sec = frame_count / fps\n",
    "            print(f\"Processing second: {current_time_sec:.2f}\")\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_rgb = enhance_image(frame_rgb, brightness=10, contrast=10)\n",
    "            frame_resized = yolo_transform(frame_rgb).unsqueeze(0).to(device)\n",
    "\n",
    "            results = yolo_model(frame_resized)\n",
    "\n",
    "            detections = []\n",
    "            for r in results:\n",
    "                boxes = r.boxes\n",
    "                for box in boxes:\n",
    "                    x1, y1, x2, y2 = box.xyxy[0]\n",
    "                    conf = box.conf[0]\n",
    "                    cls = int(box.cls[0])\n",
    "                    if cls == 0:\n",
    "                        orig_h, orig_w = frame.shape[:2]\n",
    "                        x1 = int(x1.item() * orig_w / 640)\n",
    "                        y1 = int(y1.item() * orig_h / 640)\n",
    "                        x2 = int(x2.item() * orig_w / 640)\n",
    "                        y2 = int(y2.item() * orig_h / 640)\n",
    "                        detections.append(([x1, y1, x2 - x1, y2 - y1], conf, 'person'))\n",
    "\n",
    "            if not detections:\n",
    "                no_person_detected_count += 1\n",
    "                continue\n",
    "\n",
    "            tracks = tracker.update_tracks(detections, frame=frame_rgb)\n",
    "\n",
    "            for track in tracks:\n",
    "                if not track.is_confirmed():\n",
    "                    continue\n",
    "\n",
    "                ltrb = track.to_ltrb()\n",
    "                x1, y1, x2, y2 = map(int, ltrb)\n",
    "                person_image = frame_rgb[y1:y2, x1:x2]\n",
    "\n",
    "                try:\n",
    "                    faces = mtcnn(Image.fromarray(person_image))\n",
    "                    if faces is not None and faces.shape[0] > 0:\n",
    "                        faces = faces.to(device)\n",
    "                        face_embeddings = resnet(faces).detach().cpu()\n",
    "\n",
    "                        for face_embedding in face_embeddings:\n",
    "                            if len(known_face_embeddings) == 0 or not any(torch.nn.functional.cosine_similarity(face_embedding, known_emb, dim=0) > 0.7 for known_emb in known_face_embeddings):\n",
    "                                total_unique_person_count += 1\n",
    "                                known_face_embeddings.append(face_embedding)\n",
    "\n",
    "                                face_image = Image.fromarray(person_image)\n",
    "                                face_image.save(os.path.join(output_dir, f\"unique_person_{total_unique_person_count}.jpg\"))\n",
    "                    else:\n",
    "                        no_face_detected_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in face detection/recognition: {e}\")\n",
    "                    no_face_detected_count += 1\n",
    "\n",
    "            passersby_count += len(tracks)\n",
    "            frame_count += 1\n",
    "            print(f\"Processed {frame_count} frames. Current unique people count: {total_unique_person_count}\")\n",
    "        else:\n",
    "            frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    return total_unique_person_count, known_face_embeddings, frame_count, no_face_detected_count, no_person_detected_count, passersby_count\n",
    "\n",
    "def main(video_paths):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    try:\n",
    "        yolo_model = YOLO(\"yolov8n.pt\")\n",
    "        yolo_model.to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading YOLO model: {e}\")\n",
    "        return\n",
    "\n",
    "    tracker = DeepSort(max_age=30)\n",
    "    mtcnn = MTCNN(keep_all=True, device=device)\n",
    "    resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "    output_dir = \"/content/unique_face_images\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    total_unique_person_count = 0\n",
    "    known_face_embeddings = []\n",
    "    start_time = time.time()\n",
    "    total_frame_count = 0\n",
    "    total_no_face_detected_count = 0\n",
    "    total_no_person_detected_count = 0\n",
    "    total_passersby_count = 0\n",
    "\n",
    "    for i, video_path in enumerate(video_paths):\n",
    "        print(f\"Processing video {i+1}/{len(video_paths)}: {video_path}\")\n",
    "        video_unique_count, known_face_embeddings, frame_count, no_face_count, no_person_count, passersby_count = process_video(\n",
    "            video_path, yolo_model, tracker, mtcnn, resnet, output_dir, known_face_embeddings, total_unique_person_count\n",
    "        )\n",
    "        total_unique_person_count = video_unique_count\n",
    "        total_frame_count += frame_count\n",
    "        total_no_face_detected_count += no_face_count\n",
    "        total_no_person_detected_count += no_person_count\n",
    "        total_passersby_count += passersby_count\n",
    "\n",
    "        video_output_filename = f\"video_{i+1}_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "        with open(video_output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"Video path: {video_path}\\n\")\n",
    "            f.write(f\"Unique persons: {video_unique_count}\\n\")\n",
    "            f.write(f\"Processed frames: {frame_count}\\n\")\n",
    "            f.write(f\"Frames with no face detected: {no_face_count}\\n\")\n",
    "            f.write(f\"Frames with no person detected: {no_person_count}\\n\")\n",
    "            f.write(f\"Passersby count: {passersby_count}\\n\")\n",
    "\n",
    "        print(f\"Results for video {i+1} saved to {video_output_filename}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    output_filename = f\"combined_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Total unique persons across all videos: {total_unique_person_count}\\n\")\n",
    "        f.write(f\"Total execution time: {execution_time:.2f} seconds\\n\")\n",
    "        f.write(f\"Total processed frames: {total_frame_count}\\n\")\n",
    "        f.write(f\"Total frames with no face detected: {total_no_face_detected_count}\\n\")\n",
    "        f.write(f\"Total frames with no person detected: {total_no_person_detected_count}\\n\")\n",
    "        f.write(f\"Total passersby count: {total_passersby_count}\\n\")\n",
    "        f.write(f\"Date and time: {datetime.now().strftime('%Y%m%d_%H%M%S')}\\n\")\n",
    "        f.write(f\"Face images saved at: {os.path.abspath(output_dir)}\\n\")\n",
    "\n",
    "    print(f\"Combined results saved to {output_filename}\")\n",
    "    print(f\"Face images saved at {os.path.abspath(output_dir)}\")\n",
    "\n",
    "    files.download(output_filename)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_paths = [\n",
    "        \"/content/drive/MyDrive/MIPPE記録/MAH00065.MP4\",\n",
    "        \"/content/drive/MyDrive/MIPPE記録/MAH00066.MP4\"\n",
    "    ]\n",
    "    main(video_paths)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "23a969fb6d824728aa9f504ede5819fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_287132a2e8be411982e21209a07db4dc",
      "max": 111898327,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8f8b152be5264b0da233f4f225dbf434",
      "value": 111898327
     }
    },
    "287132a2e8be411982e21209a07db4dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33bd89f72aa245ca952d9906f2e2fa2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8bff9a8eb21e4222aeb385f15eede244",
       "IPY_MODEL_23a969fb6d824728aa9f504ede5819fa",
       "IPY_MODEL_708d963d3e1649eebeb959e08a3f27fe"
      ],
      "layout": "IPY_MODEL_b6341f97a93148678f585a67bbe8b2ea"
     }
    },
    "418f6dd8e50b4ef0ab96cbc2efffbe61": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4705eafb2871441588e8b30689670863": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "708d963d3e1649eebeb959e08a3f27fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8674158cd57b4f05b45663a2f043abb2",
      "placeholder": "​",
      "style": "IPY_MODEL_4705eafb2871441588e8b30689670863",
      "value": " 107M/107M [00:00&lt;00:00, 257MB/s]"
     }
    },
    "8674158cd57b4f05b45663a2f043abb2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8bff9a8eb21e4222aeb385f15eede244": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_418f6dd8e50b4ef0ab96cbc2efffbe61",
      "placeholder": "​",
      "style": "IPY_MODEL_a4d90527f24b41df8ee69f29f2c280c1",
      "value": "100%"
     }
    },
    "8f8b152be5264b0da233f4f225dbf434": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a4d90527f24b41df8ee69f29f2c280c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b6341f97a93148678f585a67bbe8b2ea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
